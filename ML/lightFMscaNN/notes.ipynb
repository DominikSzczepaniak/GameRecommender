{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightFM & ScaNN  |  Game Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 19:01:13.878016: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-19 19:01:13.945449: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-19 19:01:14.135122: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-19 19:01:14.135756: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-19 19:01:14.864885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# -----------------=[ Load Dependencies ]=----------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scann\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------=[ Data reading ]=----------------\n",
    "\n",
    "users = pd.read_csv('./data/users.csv')\n",
    "games = pd.read_csv('./data/games.csv')\n",
    "recommendations = pd.read_csv('./data/recommendations.csv')\n",
    "gamesMetadata = pd.read_json('./data/games_metadata.json', lines=True)\n",
    "\n",
    "interactions = load_npz('./data/train_and_test.npz').tocsr()\n",
    "\n",
    "# Test users with 40% of history (This is used for testing)\n",
    "rest_test = load_npz('./data/rest_test.npz').tocsr()\n",
    "\n",
    "# Test users with 100% history (Used for getting user indicies)\n",
    "test_matrix = load_npz('./data/test_matrix.npz').tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------=[ Mappers ]=-------------------\n",
    "\n",
    "userIds = users['user_id'].unique()\n",
    "gameIds = games['app_id'].unique()\n",
    "\n",
    "mapUserId = {user_id: idx for idx, user_id in enumerate(userIds)}\n",
    "mapGameId = {game_id: idx for idx, game_id in enumerate(gameIds)}\n",
    "mapUserIndex = {idx: user_id for user_id, idx in mapUserId.items()}\n",
    "mapGameIndex = {idx: game_id for game_id, idx in mapGameId.items()}\n",
    "\n",
    "mapToTitle = lambda game_id: games[games['app_id'] == game_id]['title'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------=[ Game Features ]=-------------------\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "feature_matrix = mlb.fit_transform(gamesMetadata['tags'])\n",
    "\n",
    "feature_matrix_df = pd.DataFrame(feature_matrix, columns=mlb.classes_)\n",
    "\n",
    "dataset = Dataset()\n",
    "\n",
    "dataset.fit(\n",
    "  items=gameIds,\n",
    "  users=userIds,\n",
    "  item_features=feature_matrix_df\n",
    ")\n",
    "\n",
    "item_features = dataset.build_item_features(\n",
    "    ((row['app_id'], [tag]) for _, row in gamesMetadata.iterrows() for tag in row['tags'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------=[ Data preparation ]=-------------------\n",
    "\n",
    "def fit(model, loss, epochs=6):\n",
    "  for epoch in tqdm(range(1, epochs + 1)):\n",
    "    model.fit_partial(interactions, epochs=50, num_threads=20)\n",
    "\n",
    "    with open(f'./data/model/lightfm_{loss}.pkl', 'wb') as f:\n",
    "      pickle.dump(model, f)\n",
    "\n",
    "def loadModel(loss) -> LightFM:\n",
    "  with open(f'./data/model/lightfm_{loss}.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loadModel() missing 1 required positional argument: 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26816/462521492.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ----------------=[ Model training ]=---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: loadModel() missing 1 required positional argument: 'loss'"
     ]
    }
   ],
   "source": [
    "# ----------------=[ Model training ]=---------------\n",
    "\n",
    "model = loadModel('warp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listUserLikedGames(user_id, matrix):\n",
    "  user_index = mapUserId[user_id]\n",
    "  user_ratings = matrix[user_index].toarray()[0]\n",
    "\n",
    "  games = []\n",
    "\n",
    "  for idx, rating in enumerate(user_ratings):\n",
    "    if rating == 1:\n",
    "      games.append(mapGameIndex[idx])\n",
    "\n",
    "  return games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------=[ Prediction ]=------------------\n",
    "\n",
    "def embedUser(user_id):\n",
    "  user_games = listUserLikedGames(user_id, interactions)\n",
    "\n",
    "  if len(user_games) == 0:\n",
    "    return np.zeros(64)\n",
    "  \n",
    "  game_indices = [mapGameId[game_id] for game_id in user_games]\n",
    "  game_embeddings = model.item_embeddings[game_indices]\n",
    "\n",
    "  user_embedding = np.mean(game_embeddings, axis=0)\n",
    "  \n",
    "  return user_embedding\n",
    "\n",
    "\n",
    "\n",
    "def recommend(user_id, k):\n",
    "    searcher = scann.scann_ops_pybind.builder(model.item_embeddings, k, \"dot_product\").score_ah(1).build()\n",
    "\n",
    "    user_embedding = embedUser(user_id)\n",
    "    indices, scores = searcher.search(user_embedding)\n",
    "\n",
    "    print(indices)\n",
    "    print('chuj')\n",
    "    print(scores)\n",
    "\n",
    "    sorted_indices = np.argsort(-scores)\n",
    "    sorted_item_indices = [indices[i] for i in sorted_indices]\n",
    "\n",
    "    return [mapGameIndex[idx] for idx in sorted_item_indices]\n",
    "\n",
    "# -----------------=[ For Fun ]=------------------\n",
    "\n",
    "def similarGames(game_id, k):\n",
    "  searcher = scann.scann_ops_pybind.builder(model.item_embeddings, k, \"dot_product\").score_ah(1).build()\n",
    "  \n",
    "  game_index = mapGameId[game_id]\n",
    "  game_embedding = model.item_embeddings[game_index]\n",
    "  indecies, scores = searcher.search(game_embedding)\n",
    "  print(indecies)\n",
    "  print('chuj')\n",
    "  print(scores)\n",
    "\n",
    "  return [mapGameIndex[idx] for idx in indecies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 458/4936453 [00:00<2:42:36, 505.91it/s]2025-01-17 00:59:44.106468: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  0%|          | 2559/4936453 [00:06<2:42:53, 504.84it/s]2025-01-17 00:59:50.110993: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  0%|          | 2804/4936453 [00:09<6:25:09, 213.49it/s] 2025-01-17 00:59:52.419039: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  0%|          | 5893/4936453 [00:17<2:54:34, 470.73it/s] 2025-01-17 01:00:00.337318: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  0%|          | 7190/4936453 [00:21<2:51:24, 479.29it/s] 2025-01-17 01:00:04.776222: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  0%|          | 10386/4936453 [00:29<2:48:49, 486.29it/s]2025-01-17 01:00:12.839604: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  0%|          | 14316/4936453 [00:39<2:34:20, 531.52it/s] 2025-01-17 01:00:22.170670: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  0%|          | 14560/4936453 [00:41<6:18:32, 216.71it/s] 2025-01-17 01:00:24.384361: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 31573/4936453 [01:16<2:56:25, 463.34it/s] 2025-01-17 01:00:59.630021: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 37547/4936453 [01:30<2:57:59, 458.70it/s] 2025-01-17 01:01:13.261168: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 39795/4936453 [01:36<2:28:22, 550.04it/s] 2025-01-17 01:01:19.307566: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 43102/4936453 [01:44<2:32:15, 535.65it/s] 2025-01-17 01:01:27.369498: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 45460/4936453 [01:50<2:30:13, 542.65it/s] 2025-01-17 01:01:33.565130: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 50947/4936453 [02:02<2:41:40, 503.65it/s] 2025-01-17 01:01:45.772327: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 52058/4936453 [02:06<2:27:42, 551.14it/s] 2025-01-17 01:01:49.582732: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 52667/4936453 [02:09<2:56:31, 461.10it/s] 2025-01-17 01:01:52.407985: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 54348/4936453 [02:14<2:26:07, 556.85it/s] 2025-01-17 01:01:57.209638: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|          | 59173/4936453 [02:24<2:25:40, 558.00it/s] 2025-01-17 01:02:07.999968: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|▏         | 64969/4936453 [02:37<2:29:49, 541.90it/s] 2025-01-17 01:02:20.532100: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|▏         | 68829/4936453 [02:46<2:25:50, 556.26it/s] 2025-01-17 01:02:29.550552: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n",
      "  1%|▏         | 68878/4936453 [02:48<3:18:05, 409.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#🇺🇦 🇺🇦 🇺🇦 🇺🇦 🇺🇦 🇺🇦 🇺🇦 🇺🇦 🇺🇦 🇺🇦\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get test user ids\n",
    "test_user_ids = get_user_test_ids()\n",
    "\n",
    "historyGetter = HistoryGetter()\n",
    "\n",
    "# Empty lists to hold predictions and actuals\n",
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "interactions = interactions.tocsr()\n",
    "nigger = 0\n",
    "# Iterate through a small subset (for testing)\n",
    "for index in tqdm(list(test_user_ids)):  # Example limit for testing\n",
    "    user_id = index\n",
    "    real_at = historyGetter.get_user_actual(user_id)\n",
    "\n",
    "    if (len(real_at) > 20):\n",
    "      nigger += 1\n",
    "      predicted_at = recommend(user_id, len(real_at))\n",
    "\n",
    "      predicted.append(predicted_at)\n",
    "      actual.append(real_at)\n",
    "\n",
    "      if (nigger == 20):\n",
    "         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11665163385172614\n"
     ]
    }
   ],
   "source": [
    "print(NDCG(predicted, actual, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14453 15284 14095 15809 14535 48122 13272 48611 15268 12800]\n",
      "chuj\n",
      "[1035.6027   350.175    347.19052  342.21646  340.22684  319.3357\n",
      "  308.39273  296.45496  292.4757   289.49124]\n",
      "['ELDEN RING', 'V Rising', 'Sid Meier’s Civilization® VI', 'Dying Light 2 Stay Human', \"Tom Clancy's Rainbow Six® Siege\", 'Sekiro™: Shadows Die Twice - GOTY Edition', 'DARK SOULS™ II: Scholar of the First Sin', 'Thymesia', 'DARK SOULS™: REMASTERED', 'The Witcher® 3: Wild Hunt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 18:52:58.238448: I scann/base/single_machine_factory_scann.cc:153] Single-machine AH training with dataset size = 50872, 20 thread(s).\n"
     ]
    }
   ],
   "source": [
    "# ----------------=[ Testing Games ]=------------------\n",
    "\n",
    "game_title = 'ELDEN RING'\n",
    "\n",
    "game_id = games[games['title'] == game_title]['app_id'].values[0]\n",
    "\n",
    "# user\n",
    "\n",
    "\n",
    "predicted2 = similarGames(game_id, 10)\n",
    "\n",
    "print(list(map(mapToTitle, predicted2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26816/1326430407.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecommend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1723232\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_26816/3642906635.py\u001b[0m in \u001b[0;36mrecommend\u001b[0;34m(user_id, k)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecommend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0msearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscann_ops_pybind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dot_product\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_ah\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0muser_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedUser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "recommend(1723232, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
